# Deep Learning for Natural Language Processing 

## Homework 1 - Linear Classifier for Text Classification

This notebook implements a linear classifier for text classification using the IMDB dataset. It explores text vectorization techniques including CountVectorizer and TfidfVectorizer, and demonstrates handling of out-of-vocabulary words.

## Homework 2 - Word2Vec Implementation

This notebook implements the Word2Vec model from scratch using PyTorch, demonstrating the Skip-gram architecture and efficient vector operations through matrix multiplication. The implementation includes optimizations for training speed and numerical stability, with evaluation of word similarities on a Star Wars text corpus.

## Homework 3 - Neural Networks and SVM

This notebook implements a two-layer neural network and SVM classifier for image classification on CIFAR-10, exploring different optimization techniques and hyperparameter tuning for improved model performance.

## Homework 4 - Neural Network Text Classifier

This notebook implements a fully-connected neural network classifier for text classification using PyTorch. The implementation includes techniques like batch normalization, dropout, and L2 regularization, with a focus on proper project structure and model evaluation. The code demonstrates text preprocessing, model training with early stopping, and interactive inference capabilities.

## Homework 5 - Transformer Language Model

This notebook implements a transformer-based language model from scratch using PyTorch, starting with the implementation of multi-head self-attention mechanisms and building up to a complete transformer architecture. The implementation includes training on the Tiny Shakespeare dataset, demonstrating character-level language modeling with techniques like causal masking, positional encoding, and efficient batching. The code explores hyperparameter optimization through wandb integration, model evaluation using perplexity metrics, and text generation capabilities.
