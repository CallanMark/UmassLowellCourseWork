{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "\n",
    "'''\n",
    "Need GPU Acsess for this \n",
    "'''\n",
    "\n",
    "sys.path.append('../')  # make sure we can import transformer_lm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a transformer language model\n",
    "\n",
    "In this notebook, we will learn how to\n",
    "\n",
    "1. preprocess data for language modeling\n",
    "2. use `torch.utils.data` to handle batching in an efficient and standard way\n",
    "3. train a transformer language model\n",
    "\n",
    "Specifically, we will use the Tiny Shakespeare dataset, which contains the complete works of William Shakespeare, to train a language model. The goal of this notebook is to walk you through the steps of pre-processing the dataset and preparing it for training using the PyTorch DataLoader, creating a language model, training it and using it to generate text.\n",
    "\n",
    "We will train a character-based langauge model instead of word-based, because:\n",
    "\n",
    "1. It's faster to train it to the point that it can generate text\n",
    "2. We don't want to complicate the homework with BPE tokenization\n",
    "3. We work with a small dataset which might not be enough to train a word-based language model\n",
    "\n",
    "> Feel free to try training a word-based language model on a larger dataset, such as the WikiText-2 dataset, which is available in the hugginface datasets library.\n",
    "\n",
    "# Step 1: Load and Explore the Dataset\n",
    "The first step is to load the dataset and explore it. In this example, we will use the Tiny Shakespeare dataset, which contains the complete works of William Shakespeare. We can download the dataset from the following URL: https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "\n",
    "Feel free to use `wget` to download the dataset or just download the file manually and upload it to your Colab instance.\n",
    "\n",
    "Here's how you can use `wget` to download the dataset:\n",
    "```\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -O tiny_shakespeare.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding task 3.1: load the data and take a look\n",
    "\n",
    "Read the file to a variable named `raw_data` and print the first 1000 characters.\n",
    "\n",
    "### Grading criteria\n",
    "**(1 point max)**\n",
    "\n",
    "1 point if everything works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mark/Documents/college/NLP/Homeworks/HW4/HW5\n",
      "Vocab Length :  65\n",
      "Data has length of :  1115394\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "os.chdir(\"/Users/mark/Documents/college/NLP/Homeworks/HW4/HW5\")\n",
    "print(os.getcwd())\n",
    "\n",
    "with open(\"tiny_shakespeare.txt\", \"r\") as f:\n",
    "    raw_data = f.read()\n",
    "vocab = sorted(set(raw_data))\n",
    "print(\"Vocab Length : \" , len(vocab))\n",
    "print(\"Data has length of : \" ,len(raw_data))\n",
    "print(raw_data[:1000])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline question 3.1: raw text preprocessing\n",
    "**(1 point max, 1 extra point for creative ideas)**\n",
    "\n",
    "Think about how you can pre-process the data (in terms of modifying the text). Provde three ideas and explain why you think they are useful or not. Think about the size of the data, tokenization method (we will use character-level language model), your computational resources, and what kind of text you want to generate. Make this answer as extensive as possible.\n",
    "\n",
    "***Your answer:***\n",
    "1. Convert all text to same case (lowercase) to ensure we are uniform formatting across our data. This reduces the size of unique characthers from 52 to 26. Doing this also forces the model to focus on content and structure rather than case  \n",
    "2. Strip whitespaces to reduce the size of our dataset without sacrficing any meaningful content. This also ensures consistency across our data in terms of whitespaces. We are also more computionally efficent with this as we require less computing resources \n",
    "3. Batch processing the data will help with memory management , to do this we could divide the data up into chunks e.g 2% of the dataset at a time (1,115,394 * 0.02) e.g chunks of 22,307 length. Or we could use soem predefined fixed lengths e.g chunks of 10,000 length "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: preparing the data for the model\n",
    "\n",
    "## Coding task 3.2\n",
    "Similar to previous homeworks, where we made a vocabualry of words, we will make a vocabulary of characters.\n",
    "\n",
    "1. Make a vocabulary of all characters\n",
    "2. Make `char2idx`\n",
    "3. Make a class `Tokenizer` that stores `char2idx` and has two methods: `encode` and `decode` that encode and decode text using `char2idx` and `idx2char` dictionaries.\n",
    "   * You might find it useful to create `idx2char` dictionary inside the `__init__` method of the `Tokenizer` class.\n",
    "4. Create a `Tokenizer` object\n",
    "5. Convert the text to a list of integers using `char2idx`, assign it to a variable named `data`\n",
    "6. Print the first 100 items of `data`\n",
    "\n",
    "It's useful to have a function that converts a sequence of indices to a string. You will need it to convert the output of the model to a text when you will be generating text, but is it also very useful for **debugging** your pre-processing code.\n",
    "\n",
    "### Grading criteria\n",
    "**(2 points max)**\n",
    "\n",
    "1. 1 point for `char2idx` dictionary\n",
    "2. 1 point for `Tokenizer` class that passes the tests below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE STARTS HERE (our implementation is about 4 lines using comprehensions, but it's alright if yours is longer)\n",
    "char2idx = {char: idx for idx , char in enumerate(sorted(set(raw_data)))}\n",
    "class Tokenizer:\n",
    "    def __init__(self,char2idx):\n",
    "        self.char2idx , self.idx2char = char2idx,{idx: char for char , idx in char2idx.items()}\n",
    "    def encode(self,raw_data): return [self.char2idx[char]for char in raw_data]\n",
    "    def decode(self,token_ids): return ''.join(self.idx2char[idx]for idx in token_ids)\n",
    "# YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(char2idx)\n",
    "\n",
    "token_ids = tokenizer.encode(\"hello\")\n",
    "text = tokenizer.decode(token_ids)\n",
    "\n",
    "assert isinstance(token_ids, list), \"token_ids should be a list\"\n",
    "assert isinstance(token_ids[0], int), \"token_ids should be a list of integers\"\n",
    "assert text == \"hello\", \"decode should work correctly and return the original text\"\n",
    "\n",
    "del token_ids, text # Removed del tokenizer from here "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunk the data\n",
    "\n",
    "Our data is too long to be processed in one go. We will split it into chunks of length 128. We will use the first 128 characters to predict the next character. This is a decent length for a sequence, but you can play with it if you want.\n",
    "\n",
    "## Coding task 3.3\n",
    "\n",
    "1. Create a list of sequences of length `MAX_LEN + 1`. Each sequence should be a list of integers. You'll see why we need `+ 1` in a minute.\n",
    "   * You might need to get rid of your last example if it's shorter than `MAX_LEN + 1` characters. We need all data to be of the same length to simplify batching.\n",
    "   * In the next homework we will implement batchihg for sequences of different lengths and you are probably not going to enjoy it, it's a bit tricky.\n",
    "2. Split the data into training and validation sets. Use 90% of the data for training and 10% for validation.\n",
    "3. Make x and y pairs for your data. Remember that we want to use the first 128 characters to predict the next character. So, `x` should be the first 128 characters and `y` should be a shifted version of the same sequence, so it's the last 128 characters. Name them `train_x` and `train_y` for the training set and `val_x` and `val_y` for the validation set.\n",
    "4. Print an example from the training set. You should see that the first 128 characters are the same as the first 128 characters of the original text, and the last 128 characters are the same as the last 128 characters of the original text, shifted by one character.\n",
    "\n",
    "You can just stride using `data[i:i+128]` for each `i` in `range(0, len(data), 128)`, no need to do anything fancy. You can figure out more complex ways to do it, just do this after all the homework is done. You receive no extra points if your homework is not finished.\n",
    "\n",
    "### Grading criteria\n",
    "\n",
    "1. 1 point for `data_chunks` list and train-test split\n",
    "2. 1 point for dataset and dataloader objects\n",
    "3. Extra point for a more interesting way to chunk the text\n",
    "4. Extra point for implementing a custom dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_x length: 7841, Train_y length: 7841\n",
      "Val_x length: 872, Val_y length: 872\n",
      "Train_x[0] length: 127, Train_y[0] length: 127\n",
      "\n",
      "Example from training set:\n",
      "train_x[0]: [18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 14, 43, 44, 53, 56, 43, 1, 61, 43, 1, 54, 56, 53, 41, 43, 43, 42, 1, 39, 52, 63, 1, 44, 59, 56, 58, 46, 43, 56, 6, 1, 46, 43, 39, 56, 1, 51, 43, 1, 57, 54, 43, 39, 49, 8, 0, 0, 13, 50, 50, 10, 0, 31, 54, 43, 39, 49, 6, 1, 57, 54, 43, 39, 49, 8, 0, 0, 18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 37, 53, 59, 1, 39, 56, 43, 1, 39, 50, 50, 1, 56, 43, 57, 53, 50, 60, 43, 42, 1, 56, 39, 58, 46, 43, 56, 1, 58, 53]\n",
      "train_y[0]: [47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 14, 43, 44, 53, 56, 43, 1, 61, 43, 1, 54, 56, 53, 41, 43, 43, 42, 1, 39, 52, 63, 1, 44, 59, 56, 58, 46, 43, 56, 6, 1, 46, 43, 39, 56, 1, 51, 43, 1, 57, 54, 43, 39, 49, 8, 0, 0, 13, 50, 50, 10, 0, 31, 54, 43, 39, 49, 6, 1, 57, 54, 43, 39, 49, 8, 0, 0, 18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 37, 53, 59, 1, 39, 56, 43, 1, 39, 50, 50, 1, 56, 43, 57, 53, 50, 60, 43, 42, 1, 56, 39, 58, 46, 43, 56, 1, 58, 53, 1]\n",
      "Decoded train_x[0]:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to'\n",
      "Decoded train_y[0]:  'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to '\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 128\n",
    "\n",
    "# YOUR CODE STARTS HERE (our implementation is about 13 lines, but it's alright if yours is different)\n",
    "# This could probably be done better  \n",
    "tokenizer = Tokenizer(char2idx)\n",
    "data = tokenizer.encode(raw_data)\n",
    "data_chunks = [data[i:i + MAX_LEN] for i in range (0,len(data)- (MAX_LEN +1) , MAX_LEN)]\n",
    "if int(len(data_chunks[-1])) < int((MAX_LEN+1)):\n",
    "    data_chunks = data_chunks[:-1]\n",
    "# Split into training and validation sets \n",
    "split_indice = int(0.9 * len(data_chunks))\n",
    "train_chunks = data_chunks[:split_indice]\n",
    "test_chunks = data_chunks[split_indice:]\n",
    "\n",
    "train_x = [chunk[:-1]for chunk in train_chunks]\n",
    "train_y = [chunk[1:]for chunk in train_chunks]\n",
    "val_x = [chunk[:-1] for chunk in test_chunks ]\n",
    "val_y = [chunk[1:] for chunk in test_chunks] \n",
    "\n",
    "print(f\"Train_x length: {len(train_x)}, Train_y length: {len(train_y)}\")\n",
    "print(f\"Val_x length: {len(val_x)}, Val_y length: {len(val_y)}\")\n",
    "print(f\"Train_x[0] length: {len(train_x[0])}, Train_y[0] length: {len(train_y[0])}\")\n",
    "\n",
    "print(\"\\nExample from training set:\")\n",
    "print(f\"train_x[0]: {train_x[0]}\")\n",
    "print(f\"train_y[0]: {train_y[0]}\")\n",
    "\n",
    "print(\"Decoded train_x[0]: \", repr(tokenizer.decode(train_x[0])))\n",
    "print(\"Decoded train_y[0]: \", repr(tokenizer.decode(train_y[0])))\n",
    "# YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using `torch.utils.data`\n",
    "\n",
    "We will use `torch.utils.data.Dataset` to create a dataset object that will be used to create a `torch.utils.data.DataLoader` object. The `DataLoader` object will be used to create batches of data.\n",
    "\n",
    "## Coding task 3.4\n",
    "\n",
    "Your task is to learn how to use `torch.utils.data.Dataset` and `torch.utils.data.DataLoader` classes and to apply them to our data.\n",
    "\n",
    "1. Convert your data to tensors of type long\n",
    "1. Create a `torch.utils.data.Dataset` object for each train and test data. Name them `train_dataset` and `val_dataset`. You can use the `TensorDataset` class for this or make a new class that inherits from `torch.utils.data.Dataset` and implements the `__getitem__` and `__len__` methods.\n",
    "2. Try indexing `train_dataset` to get a single example and decode it using `tokenizer.decode()`. What does it contain? Use tokenizer to decode one example (both x and y). Does it look like a valid text? Are the targets shifted by one character?\n",
    "1. Use the `DataLoader` class to create `train_loader` and `val_loader` objects. It will shuffle and batch data for you. You can use the following parameters:\n",
    "   * `dataset` - the dataset object you created in the previous step\n",
    "   * `batch_size` - your choice!\n",
    "   * `shuffle` - True for training data, False for validation data\n",
    "   * `num_workers` - 8, number of CPU cores to use for batch preparation\n",
    "3. Try iterating over `train_loader` and print the shapes of the batches.\n",
    "    * You can use `break` to stop the loop after the first iteration.\n",
    "4. Try decoding a batch that you get from `train_loader`. Does it look like a valid text? Are the targets shifted by one character?\n",
    "\n",
    "Learn more about data feeding in pytorch here: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "\n",
    "\n",
    "**NOTE:**\n",
    "1. `TensorDataset` returns a tuple of tensors. Usually these are `(x, y)` pairs, where `x` is the input and `y` is the target. In our case, `x` is the input sequence and `y` is the same sequence shifted by one character. This is how we will train our language model. We will use the first 128 characters to predict the next character.\n",
    "1. You need to convert your pytorch tensor into a python list in order to use `tokenizer.decode()`. Feel free to do it in-place or modify the `decode` method of the `Tokenizer` class to accept **BOTH** python lists and pytorch tensors. You can check what datatype you have using `isinstance()` function.\n",
    "2. Printing might look a bit weird because you have a lot of `\\n` in the data. It is alright, just be careful when you are verifying that your data is correct.\n",
    "\n",
    "### Grading criteria\n",
    "\n",
    "* 1 point for `train_dataset` and `val_dataset` objects\n",
    "* 1 point if each test is written and passed:\n",
    "  * train dataset element is correctly processed and x and y corespond to the correct characters\n",
    "  * printed the shapes of the items that you get from `train_loader`\n",
    "  * decoded a batch from `train_loader` and printed the decoded text and it is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shapes from train loader : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(159) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(161) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(163) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(164) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(165) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(166) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(167) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(168) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(169) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(170) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_batch shape :  {torch.Size([3, 127])}\n",
      "y_batch shape :  {torch.Size([3, 127])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(171) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(172) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(173) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(174) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(175) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(177) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(178) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(179) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "tensor(43)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_batch , y_batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(BATCH_SIZE):\n\u001b[0;32m---> 25\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample \u001b[39m\u001b[38;5;124m\"\u001b[39m , {i} , \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m x: \u001b[39m\u001b[38;5;124m\"\u001b[39m , {\u001b[38;5;28mrepr\u001b[39m(\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)})\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample \u001b[39m\u001b[38;5;124m\"\u001b[39m , {i} , \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m y : \u001b[39m\u001b[38;5;124m\"\u001b[39m , {\u001b[38;5;28mrepr\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mdecode(y_batch[i]))})\n",
      "Cell \u001b[0;32mIn[26], line 7\u001b[0m, in \u001b[0;36mTokenizer.decode\u001b[0;34m(self, token_ids)\u001b[0m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m,token_ids): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43midx2char\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 7\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m,token_ids): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43midx2char\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m token_ids)\n",
      "\u001b[0;31mKeyError\u001b[0m: tensor(43)"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 3  # think about a better batch size for training, this is just a placeholder\n",
    "\n",
    "# YOUR CODE STARTS HERE (our implementation is about 13 lines)\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# Convert to tensors\n",
    "train_x_tensor = torch.tensor(train_x,dtype=torch.long)\n",
    "train_y_tensor = torch.tensor(train_y,dtype=torch.long)\n",
    "val_x_tensor = torch.tensor(val_x,dtype=torch.long)\n",
    "val_y_tensor = torch.tensor(val_y,dtype=torch.long)\n",
    "#Dataset objects \n",
    "train_dataset = TensorDataset(train_x_tensor,train_y_tensor)\n",
    "val_dataset = TensorDataset(val_x_tensor,val_y_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size = BATCH_SIZE , shuffle=True , num_workers = 8)\n",
    "val_loader = DataLoader(val_dataset,batch_size = BATCH_SIZE,shuffle= False , num_workers = 8)\n",
    "\n",
    "print(\"Batch shapes from train loader : \")\n",
    "for x_batch , y_batch in train_loader:\n",
    "    print(\"x_batch shape : \" ,{x_batch.shape})\n",
    "    print(\"y_batch shape : \" , {y_batch.shape})\n",
    "    break\n",
    "\n",
    "for x_batch , y_batch in train_loader:\n",
    "    for i in range(BATCH_SIZE):\n",
    "        print(\"Sample \" , {i} , \" x: \" , {repr(tokenizer.decode(x_batch[i]))})\n",
    "        print(\"Sample \" , {i} , \" y : \" , {repr(tokenizer.decode(y_batch[i]))})\n",
    "\n",
    "# YOUR CODE ENDS HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Transformer model\n",
    "\n",
    "Import your `TransformerLM` model from `modeling_transormer` file and train it on the data you prepared above.\n",
    "You know the drill: define a model, an optimizer, and a training loop, log everything to wandb.\n",
    "You can also save your model using `TransformerLM.save_pretrained()` method and load it using `TransformerLM.from_pretrained()` method in case you want to.\n",
    "\n",
    "### Tricky part\n",
    "\n",
    "In PyTorch, `F.cross_entropy` expects the logits to be of shape `(batch_size, num_classes)` and the targets to be of shape `(batch_size,)` containing the class indices. In our case, the logits tensor has the shape `(batch_size, seq_len, num_classes)` and the targets are of shape `(batch_size, seq_len)`. We need to reshape the input and the targets to make them compatible with `F.cross_entropy`. You can do it like this:\n",
    "\n",
    "```python\n",
    "bs, seq_len, num_classes = logits.shape\n",
    "logits = logits.reshape(bs * seq_len, num_classes)\n",
    "targets = targets.reshape(bs * seq_len)\n",
    "```\n",
    "\n",
    "or, equivalently, like this:\n",
    "\n",
    "```python\n",
    "logits = logits.view(-1, num_classes)\n",
    "targets = targets.view(-1)\n",
    "```\n",
    "\n",
    "Try monitoring your GPU consumption and max it out. The more efficient your code is, the faster your model will train.\n",
    "During training log your loss and and accuracy. You can only log accuracy every 100 batches or so, because it is a bit slow to compute. You can also log the learning rate.\n",
    "During evlauation you just need to log the perplexity, the loss, and accuracy. Perplexity is just `exp(loss)`.\n",
    "Accuracy is not the most standard metric for language models, but it is very intererpretable and easy to compute. Don't expect it to be high, though.\n",
    "Be mindful how frequenly you evaluate your model. You don't want to evaluate it too often, because it will slow down your training loop.\n",
    "\n",
    "> You can also log the number of batches you process in one second (throughput) as a measure of efficiency. It is not required, but it is a good idea to monitor it.\n",
    "\n",
    "## Coding task 3.5\n",
    "\n",
    "Make a training loop and train your model.\n",
    "\n",
    "### Grading criteria\n",
    "**(5 points + extra points)**\n",
    "\n",
    "* 2 points for trainig loop\n",
    "* 1 point for using the GPU\n",
    "* 1 point for evaluation loop (we recommend to make it into a separate function to make your code more readable)\n",
    "* 1 point for wandb logging of train loss, eval loss, train accuracy, eval accuracy, eval perplexity. You can also log the learning rate, but it is not required.\n",
    "* -1 point if forget to zero your gradients between batches\n",
    "* -1 point if your forget to put your model to evaluation mode during evaluation and back to training mode during training\n",
    "* Extra point for using a learning rate scheduler\n",
    "* Extra point for any other improvements to the training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(485) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(486) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(487) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(488) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(489) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(490) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(491) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(492) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(640) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(641) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(642) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(643) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(644) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(645) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(646) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(647) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(659) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(660) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(661) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(662) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(664) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(665) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(666) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(667) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(905) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(907) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(908) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(909) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(910) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(911) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(912) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(913) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(928) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(929) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(930) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(931) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(932) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(933) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(934) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(935) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     38\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 39\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(vocab))\n\u001b[1;32m     41\u001b[0m targets \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/venv-hw3/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/venv-hw3/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/college/NLP/Homeworks/HW4/HW5/transformer_lm/modeling_transformer.py:217\u001b[0m, in \u001b[0;36mTransformerLM.forward\u001b[0;34m(self, input_ids)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m input_ids\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput should be of size [batch_size, seq_len]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# Task 2.8 (1 point)\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;66;03m# Implement Transformer Language Model\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# Remember that Transformer Language Model is composed of:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# 3. Output Layer to produce logits over the classes (our vocabulary in case of language modeling)\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# YOUR CODE STARTS HERE (our implementation is 2 lines)\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer(x)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/venv-hw3/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/venv-hw3/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/college/NLP/Homeworks/HW4/HW5/transformer_lm/modeling_transformer.py:177\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, input_ids)\u001b[0m\n\u001b[1;32m    175\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 177\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/anaconda3/envs/venv-hw3/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/venv-hw3/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/college/NLP/Homeworks/HW4/HW5/transformer_lm/modeling_transformer.py:70\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     68\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m     69\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x)\n\u001b[0;32m---> 70\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mffn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m x)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03mMore verbose option \u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03mresidual = x  # [bs, seq, hidden]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03mTODO: Review which is best \u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# YOUR CODE ENDS HERE\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/venv-hw3/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/venv-hw3/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/venv-hw3/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/venv-hw3/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/venv-hw3/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/venv-hw3/lib/python3.10/site-packages/torch/nn/modules/dropout.py:70\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/venv-hw3/lib/python3.10/site-packages/torch/nn/functional.py:1425\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1423\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1424\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m-> 1425\u001b[0m     _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1426\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformer_lm.modeling_transformer import TransformerLM\n",
    "import wandb\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import time \n",
    "# YOUR CODE STARTS HERE\n",
    "wandb.init(project=\"transformer_shakespeare\", config={\"batch_size\": 3, \"max_len\": 128})\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TransformerLM(num_layers=4, hidden=256, num_heads=8, fcn_hidden=512, vocab_size=len(vocab), max_seq_len=128, dropout=0.1).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=0.001)\n",
    "scheduler = StepLR(optimizer, step_size=1000, gamma=0.9)\n",
    "\n",
    "def evaluate(model, val_loader, device):\n",
    "    model.eval()\n",
    "    total_loss, total_acc, total_count = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            logits = logits.view(-1, len(vocab))\n",
    "            targets = y.view(-1)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            total_acc += (logits.argmax(dim=-1) == targets).sum().item()\n",
    "            total_count += targets.size(0)\n",
    "    avg_loss = total_loss / len(val_loader.dataset)\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
    "    accuracy = total_acc / total_count\n",
    "    model.train()\n",
    "    return avg_loss, accuracy, perplexity\n",
    "\n",
    "model.train()\n",
    "for epoch in range(5):\n",
    "    start_time = time.time()\n",
    "    total_loss, total_acc, total_count = 0, 0, 0\n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        logits = logits.view(-1, len(vocab))\n",
    "        targets = y.view(-1)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        if i % 100 == 0:\n",
    "            total_acc += (logits.argmax(dim=-1) == targets).sum().item()\n",
    "            total_count += targets.size(0)\n",
    "            wandb.log({\"train_loss\": loss.item(), \"train_accuracy\": total_acc / total_count if total_count > 0 else 0, \"throughput\": 100 / (time.time() - start_time + 1e-6)})\n",
    "            start_time = time.time()\n",
    "    scheduler.step()\n",
    "    avg_train_loss = total_loss / len(train_loader.dataset)\n",
    "    eval_loss, eval_acc, eval_perp = evaluate(model, val_loader, device)\n",
    "    wandb.log({\"epoch\": epoch, \"avg_train_loss\": avg_train_loss, \"eval_loss\": eval_loss, \"eval_accuracy\": eval_acc, \"eval_perplexity\": eval_perp, \"learning_rate\": scheduler.get_last_lr()[0]})\n",
    "\n",
    "model.save_pretrained(\"transformer_shakespeare\")\n",
    "# YOUR CODE ENDS HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate text using your model\n",
    "\n",
    "Now it's time to see what this model can do. Implement a generation function.\n",
    "The idea is to start with some prefix text, predict the next character, append it to the prefix, and repeat the process.\n",
    "You can stop generating text when you reach MAX_LEN tokens.\n",
    "\n",
    "Use `torch.no_grad()` context manager to make sure that you don't compute gradients during generation, or it will blow up your GPU memory.\n",
    "\n",
    "## Coding task 3.6\n",
    "\n",
    "Implement a generation function that accepts a prefix text and generates the next tokens up to MAX_LEN.\n",
    "\n",
    "### Grading criteria\n",
    "**(2 points)**\n",
    "\n",
    "* 2 points for generation function\n",
    "* -1 point if you forget to put your model to evaluation mode during generation and back to training mode after generation or if you forget to use `torch.no_grad()` context manager, or if you are not using the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE STARTS HERE (our implementation is about 10 lines)\n",
    "\n",
    "# YOUR CODE ENDS HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring hyperparameters and understanding Transformers\n",
    "\n",
    "Train at least 10 models with different hyperparameters and compare them using wandb. Write a short report (500-1000 words).\n",
    "\n",
    "\n",
    "### Grading criteria\n",
    "**(5 points max + extra points)**\n",
    "\n",
    "* 4 points for training 10+ models. (5-9 models = 2 points, 1-4 models = 1 point)\n",
    "* 1 point for training report that describes what you did and what you learned about the hyperparameters and efficient training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-hw3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
